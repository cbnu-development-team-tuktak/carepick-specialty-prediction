import os
import json
import pandas as pd
from sklearn.preprocessing import LabelEncoder
import shutil
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

import torch
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments
)
from sklearn.metrics import accuracy_score, f1_score
from sklearn.metrics import top_k_accuracy_score

# âš™ï¸ ì„¤ì •
MODEL_NAME = "madatnlp/km-bert"
TRAIN_DIR = "./drive/MyDrive/train_data"
TEST_DIR = "./drive/MyDrive/test_data"
BATCH_SIZE = 32
EPOCHS = 5

# âœ… 1. ë°ì´í„° ë¡œë”© í•¨ìˆ˜
def load_data(data_dir):
    texts, labels = [], []
    for fname in os.listdir(data_dir):
        if not fname.endswith(".json"):
            continue
        with open(os.path.join(data_dir, fname), encoding="utf-8") as f:
            docs = json.load(f)
            for doc in docs:
                text = doc["title"] + " " + doc["content"]
                label = doc["department"].strip()
                texts.append(text)
                labels.append(label)
    return pd.DataFrame({"text": texts, "label": labels})

# âœ… 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
train_df = load_data(TRAIN_DIR)
test_df = load_data(TEST_DIR)

# âœ… 3. ë¼ë²¨ ì¸ì½”ë”©
le = LabelEncoder()
train_df["label_id"] = le.fit_transform(train_df["label"])
test_df["label_id"] = le.transform(test_df["label"])
NUM_LABELS = len(le.classes_)

# âœ… 4. Dataset ë³€í™˜ ë° í† í¬ë‚˜ì´ì§•
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def tokenize(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=256)

train_dataset = Dataset.from_pandas(train_df[["text", "label_id"]])
test_dataset = Dataset.from_pandas(test_df[["text", "label_id"]])
train_dataset = train_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)

train_dataset = train_dataset.rename_column("label_id", "labels")
test_dataset = test_dataset.rename_column("label_id", "labels")
train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
test_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

# âœ… 5. ëª¨ë¸ ë¡œë”©
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)

# âœ… 6. í‰ê°€ ë©”íŠ¸ë¦­
def compute_metrics(p):
    preds = torch.tensor(p.predictions)
    top1_preds = torch.argmax(preds, dim=1)
    labels = torch.tensor(p.label_ids)
    top3_acc = top_k_accuracy_score(labels.numpy(), preds.numpy(), k=3)

    return {
        "accuracy": accuracy_score(labels, top1_preds),
        "f1_macro": f1_score(labels, top1_preds, average="macro"),
        "top3_accuracy": top3_acc
    }

# âœ… 7. í•™ìŠµ ì¸ì
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="steps",
    save_steps=500,
    save_total_limit=2,
    learning_rate=2e-5,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    num_train_epochs=EPOCHS,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=100,
    disable_tqdm=False,
    load_best_model_at_end=False,
    report_to="none"
)

# âœ… 8. Trainer ìƒì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer
)

# âœ… 9. í•™ìŠµ ë° ì €ì¥
if __name__ == "__main__":
    resume_path = "./results/checkpoint-6000" if os.path.exists("./results/checkpoint-6000") else None
    trainer.train(resume_from_checkpoint=resume_path)

    local_output_dir = "./trained_kmbert_model"
    trainer.save_model(local_output_dir)
    tokenizer.save_pretrained(local_output_dir)

    print("âœ… í•™ìŠµ ì™„ë£Œ ë° ë¡œì»¬ ì €ì¥ ì™„ë£Œ")

    # âœ… Google Drive ì €ì¥
    drive_output_dir = "/content/drive/MyDrive/kmbert_saved_model"
    os.makedirs(drive_output_dir, exist_ok=True)

    for file_name in os.listdir(local_output_dir):
      src = os.path.join(local_output_dir, file_name)
      dst = os.path.join(drive_output_dir, file_name)
      if os.path.isfile(src):
          shutil.copy2(src, dst)  # âœ… ì•ˆì „í•˜ê²Œ ë³µì‚¬

    print(f"âœ… ëª¨ë¸ì´ Google Driveì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {drive_output_dir}")

    # âœ… ìµœì¢… ì„±ëŠ¥ í‰ê°€
    metrics = trainer.evaluate()
    print("\nğŸ“Š ìµœì¢… ì„±ëŠ¥ ì§€í‘œ:")
    for key, value in metrics.items():
        print(f"{key:<15}: {value:.4f}")

    # âœ… ì˜ˆì¸¡ ê²°ê³¼ ì¶”ì¶œ
    predictions = trainer.predict(test_dataset)
    preds = np.argmax(predictions.predictions, axis=1)
    labels = predictions.label_ids

    # âœ… ì§„ë£Œê³¼ë³„ ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥
    print("\nğŸ“‹ ì§„ë£Œê³¼ë³„ ì„±ëŠ¥ ë³´ê³ ì„œ (classification_report):")
    print(classification_report(labels, preds, target_names=le.classes_))

    # âœ… Confusion Matrix ì¶œë ¥
    cm = confusion_matrix(labels, preds)
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt="d", xticklabels=le.classes_, yticklabels=le.classes_, cmap="Blues")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title("Confusion Matrix")
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()
