# ë„¤ì´ë²„ ì§€ì‹ì¸ì˜ ë¬¸ì„œ ê³ ìœ ë²ˆí˜¸ì¸ docIdë¥¼ ì§ì ‘ ì „ìˆ˜ì¡°ì‚¬í•˜ì—¬ í¬ë¡¤ë§í•˜ëŠ” ì½”ë“œ
# ì“°ë ˆë“œ ê¸°ë²•ì„ ì´ìš©í•˜ì—¬ ë³‘ë ¬ í¬ë¡¤ë§ì„ ì‹œë„í•˜ì˜€ìŒ
# í˜„ì¬ëŠ” ì“°ì§€ ì•ŠìŒ

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import json
import time
import random
from concurrent.futures import ThreadPoolExecutor, TimeoutError, as_completed

# âœ… ëŒ€ìƒ ì§„ë£Œê³¼ ë¦¬ìŠ¤íŠ¸
TARGET_DEPARTMENTS = {
    "ì‹¬ì¥ë‚´ê³¼", "ë‚´ë¶„ë¹„ë‚´ê³¼", "ì†Œí™”ê¸°ë‚´ê³¼", "í˜ˆì•¡ì¢…ì–‘ë‚´ê³¼", "ê°ì—¼ë‚´ê³¼", "ì‹ ì¥ë‚´ê³¼", "í˜¸í¡ê¸°ë‚´ê³¼", 
    "ë¥˜ë§ˆí‹°ìŠ¤ë‚´ê³¼" "ì´ë¹„ì¸í›„ê³¼", "ì™¸ê³¼", "ëŒ€ì¥, í•­ë¬¸ ì™¸ê³¼", "í‰ë¶€ì™¸ê³¼", "ì •í˜•ì™¸ê³¼", "ì‹ ê²½ì™¸ê³¼",
    "ì‹ ê²½ê³¼", "ì •ì‹ ê±´ê°•ì˜í•™ê³¼", "ì„±í˜•ì™¸ê³¼", "í”¼ë¶€ê³¼", "ì•ˆê³¼", "ë¹„ë‡¨ì˜í•™ê³¼", "ì‚°ë¶€ì¸ê³¼",
    "ì†Œì•„ì²­ì†Œë…„ê³¼", "ê°€ì •ì˜í•™ê³¼", "ì˜ìƒì˜í•™ê³¼", "ë§ˆì·¨í†µì¦ì˜í•™ê³¼", "ì¬í™œì˜í•™ê³¼",
    "ì‘ê¸‰ì˜í•™ê³¼",
    "ì¶©ì¹˜, ì¹˜ì•„ì§ˆí™˜", "ì‡ëª¸ì§ˆí™˜", "ì¹˜ì•„êµì •", "ì˜ì¹˜, ì„í”Œë€íŠ¸", "ì¹˜ì•„ ìœ ì§€, ê´€ë¦¬"
}

# âœ… ì¹˜ê³¼ í•˜ìœ„ ì§„ë£Œê³¼
DENTAL_SUBFIELDS = {
    "ì¶©ì¹˜, ì¹˜ì•„ì§ˆí™˜", "ì‡ëª¸ì§ˆí™˜", "ì¹˜ì•„êµì •", "ì˜ì¹˜, ì„í”Œë€íŠ¸", "ì¹˜ì•„ ìœ ì§€ , ê´€ë¦¬"
}

# âœ… ìœ íš¨ ë‚ ì§œì¸ì§€ í™•ì¸
def is_valid_date(date_str):
    try:
        if "ì „" in date_str:
            return False
        date_obj = datetime.strptime(date_str.strip(), "%Y.%m.%d.")
        return datetime(2024, 1, 1) <= date_obj <= datetime(2024, 12, 31)
    except:
        return False

# âœ… ë‹¨ì¼ docId í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_doc(doc_id):
    url = f"https://kin.naver.com/qna/detail.naver?d1id=7&dirId=70106&docId={doc_id}"
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=3)
        if res.status_code != 200:
            return None

        soup = BeautifulSoup(res.text, "html.parser")

        tag_list = soup.select_one("div.tagList a")
        actual_department = tag_list.get_text(strip=True) if tag_list else None
        if actual_department not in TARGET_DEPARTMENTS:
            print(f"docId {doc_id}: ì§„ë£Œê³¼ í•„í„°ë§ìœ¼ë¡œ ì œì™¸ë¨ â†’ ì‹¤ì œ: {actual_department}")
            return None

        department = "ì¹˜ê³¼" if actual_department in DENTAL_SUBFIELDS else actual_department

        title_tag = soup.select_one("div.endTitleSection")
        title = title_tag.get_text(strip=True) if title_tag else None
        if not title:
            return None

        date_spans = soup.select("div.userInfo.userInfo__bullet span")
        date_str = date_spans[2].get_text(strip=True) if len(date_spans) >= 3 else None
        date_str = date_str[3:].strip() if date_str else None

        content_tag = soup.select_one("div.questionDetail")
        content = content_tag.get_text(separator="\n", strip=True) if content_tag else None
        if not content:
            return None

        print(f"docId {doc_id} ì €ì¥ë¨")
        print(f"ì œëª©: {title}")
        print(f"ì‘ì„±ì¼: {date_str}")
        print(f"ë³¸ë¬¸: {content[:80]}...")  # ë„ˆë¬´ ê¸¸ë©´ ì• 80ìë§Œ ì¶œë ¥

        return {
            "title": title,
            "content": content,
            "department": department,
            "date": date_str
        }

    except Exception as e:
        print(f"âŒ docId {doc_id} ì‹¤íŒ¨: {e}")
        return None
    finally:
        time.sleep(random.uniform(0.2, 0.4))

# âœ… ì•ˆì „í•œ wrapper
def safe_crawl(doc_id):
    try:
        return crawl_doc(doc_id)
    except Exception as e:
        print(f"âŒ ë‚´ë¶€ ì‹¤íŒ¨ docId {doc_id}: {e}")
        return None

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    print("í¬ë¡¤ë§ ì‹œì‘")

    start_doc_id = 461390338
    end_doc_id = 461787760
    doc_ids = list(range(start_doc_id, end_doc_id + 1))

    results = []
    with ThreadPoolExecutor(max_workers=3) as executor:  # âœ… Thread ê¸°ë°˜
        futures = {executor.submit(safe_crawl, doc_id): doc_id for doc_id in doc_ids}

        for future in as_completed(futures):
            doc_id = futures[future]
            try:
                result = future.result(timeout=8)
                if result:
                    results.append(result)
            except TimeoutError:
                print(f"â±ï¸ íƒ€ì„ì•„ì›ƒ: docId {doc_id}")
            except Exception as e:
                print(f"âš ï¸ ì˜ˆì™¸ ë°œìƒ docId {doc_id}: {e}")

    with open("medical_questions_2024.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ ì´ {len(results)}ê±´ ì €ì¥ ì™„ë£Œ")
