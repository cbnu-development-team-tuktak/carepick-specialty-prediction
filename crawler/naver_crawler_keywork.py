# ë„¤ì´ë²„ ì§€ì‹ì¸ì— "ì•ˆë…•í•˜ì„¸ìš”" í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í•œ í›„ ì„¤ì •í•œ ê¸°ê°„ ë™ì•ˆ ì‘ì„±ëœ ê¸€ì„ í¬ë¡¤ë§í•˜ëŠ” ì½”ë“œ

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import calendar
import time
import json

# ì§„ë£Œê³¼ ë§¤í•‘ (dirId)
# ì—¬ê¸°ì— ì‘ì„±ëœ ì§„ë£Œê³¼ì— ëŒ€í•´ í¬ë¡¤ë§ì„ ìˆ˜í–‰í•¨
DEPARTMENTS = {
    "ì¹˜ê³¼": "702"
}

# í¬ë¡¤ë§í•  ì§„ë£Œê³¼ ID
DIR_ID = "70119" # urlì„ ì™„ì„±í•˜ê¸° ìœ„í•´ ì„ì˜ë¡œ ë„£ì€ ì§„ë£Œê³¼
QUERY = "ì•ˆë…•í•˜ì„¸ìš”"
BASE_URL = "https://kin.naver.com/search/list.nhn"
HEADERS = {"User-Agent": "Mozilla/5.0"}
MAX_POSTS_PER_MONTH = 333  # ì›”ë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ìˆ˜

def is_valid_date(date_str, start_dt, end_dt):
    try:
        if "ì „" in date_str:
            return False
        date_obj = datetime.strptime(date_str.strip().rstrip("."), "%Y.%m.%d")
        return start_dt.date() <= date_obj.date() <= end_dt.date()
    except Exception as e:
        print(f"âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: '{date_str}' -> {e}")
        return False

def get_detail_content(url, start_dt, end_dt):
    try:
        res = requests.get(url, headers=HEADERS, timeout=2)
        if res.status_code != 200:
            return "ERROR"

        soup = BeautifulSoup(res.text, "html.parser")

        date_spans = soup.select("div.userInfo.userInfo__bullet span")
        if not date_spans:
            return "ERROR"

        if len(date_spans) == 4:
            date_str_raw = date_spans[2].get_text(strip=True)
        elif len(date_spans) == 3:
            date_str_raw = date_spans[1].get_text(strip=True)
        else:
            return "ERROR"
        
        if date_str_raw.startswith("ëŒì˜¬ì‘ì„±ì¼"):
            date_str_raw = date_str_raw.replace("ëŒì˜¬ì‘ì„±ì¼", "").strip()

        if date_str_raw.startswith("ì‘ì„±ì¼"):
            date_str_raw = date_str_raw.replace("ì‘ì„±ì¼", "").strip()

        if not is_valid_date(date_str_raw, start_dt, end_dt):
            return "OUT_OF_RANGE"

        title_tag = soup.select_one("div.endTitleSection")
        title = title_tag.get_text(strip=True) if title_tag else None
        if title and title.startswith("ì§ˆë¬¸"):
            title = title[len("ì§ˆë¬¸"):].strip()

        content_tag = soup.select_one("div.questionDetail")
        content = content_tag.get_text(separator="\n", strip=True) if content_tag else None

        tag_list = soup.select_one("div.tagList a")
        department = tag_list.get_text(strip=True) if tag_list else None

        if not all([title, content, department]):
            return "ERROR"

        return {
            "title": title,
            "content": content,
            "department": department,
            "date": date_str_raw
        }

    except Exception as e:
        print(f"âŒ ìƒì„¸ í˜ì´ì§€ ì˜¤ë¥˜: {url} -> {e}")
        return "ERROR"

def crawl_month(year, month, dir_id):
    start_dt = datetime(year, month, 1)
    last_day = calendar.monthrange(year, month)[1]
    end_dt = datetime(year, month, last_day)

    total_results = []
    page = 1

    while len(total_results) < MAX_POSTS_PER_MONTH:
        period_str = f"{start_dt.strftime('%Y.%m.%d.')}|{end_dt.strftime('%Y.%m.%d.')}"
        params = {
            "sort": "date",
            "query": QUERY,
            "period": period_str,
            "section": "qna",
            "dirId": dir_id,
            "page": page
        }
        print(f"{period_str} / page {page} ìš”ì²­ ì¤‘...")

        try:
            res = requests.get(BASE_URL, headers=HEADERS, params=params, timeout=3)
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ ìš”ì²­ ì˜¤ë¥˜: {e}")
            break

        soup = BeautifulSoup(res.text, "html.parser")
        items = soup.select("ul.basic1 > li")
        if not items:
            break

        for item in items:
            if len(total_results) >= MAX_POSTS_PER_MONTH:
                break

            a_tag = item.select_one("dl dt > a")
            href = a_tag.get("href") if a_tag else None
            if href:
                detail_url = href if href.startswith("http") else f"https://kin.naver.com{href}"
                result = get_detail_content(detail_url, start_dt, end_dt)

                if result == "OUT_OF_RANGE":
                    return total_results
                elif result == "ERROR":
                    continue
                else:
                    total_results.append(result)

        page += 1
        time.sleep(0.3)

    return total_results

if __name__ == "__main__":
    for dept_name, dir_id in DEPARTMENTS.items():
        all_results = []
        print(f"\n\n======= {dept_name} ìˆ˜ì§‘ ì‹œì‘ =======")
        for month in range(1, 4):
            monthly_results = crawl_month(2025, month, dir_id)
            all_results.extend(monthly_results)
            time.sleep(1)

        filename = f"{dept_name}_test.json"
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… {dept_name} ì €ì¥ ì™„ë£Œ: {filename} (ì´ {len(all_results)}ê±´)\n")

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ í¬ë¡¤ë§í•  ë•Œ ì“¸ ì½”ë“œ
# def crawl_month(year, month):
#     print(f"\nğŸ“… {year}ë…„ {month}ì›” í¬ë¡¤ë§ ì‹œì‘")

#     start_dt = datetime(year, month, 1)
#     last_day = calendar.monthrange(year, month)[1]
#     end_dt = datetime(year, month, last_day)

#     total_results = []
#     page = 1

#     while len(total_results) < MAX_POSTS_PER_MONTH:
#         period_str = f"{start_dt.strftime('%Y.%m.%d.')}|{end_dt.strftime('%Y.%m.%d.')}"
#         params = {
#             "sort": "date",
#             "query": QUERY,
#             "period": period_str,
#             "section": "qna",
#             "dirId": DIR_ID,
#             "page": page
#         }
#         print(f"{period_str} / page {page} ìš”ì²­ ì¤‘...")

#         try:
#             res = requests.get(BASE_URL, headers=HEADERS, params=params, timeout=3)
#         except Exception as e:
#             print(f"âŒ í˜ì´ì§€ ìš”ì²­ ì˜¤ë¥˜: {e}")
#             break

#         soup = BeautifulSoup(res.text, "html.parser")
#         items = soup.select("ul.basic1 > li")
#         if not items:
#             print("ê²Œì‹œë¬¼ ì—†ìŒ: ì¢…ë£Œ")
#             break

#         for item in items:
#             if len(total_results) >= MAX_POSTS_PER_MONTH:
#                 break

#             dl_tag = item.select_one("dl")
#             a_tag = dl_tag.select_one("dt > a") if dl_tag else None
#             href = a_tag.get("href") if a_tag else None
#             if href:
#                 detail_url = href if href.startswith("http") else f"https://kin.naver.com{href}"
#                 print(detail_url)
#                 result = get_detail_content(detail_url, start_dt, end_dt)

#                 if result == "OUT_OF_RANGE":
#                     print("â›” ë‚ ì§œ ì´ˆê³¼ ê¸€ â†’ ë‹¤ìŒ êµ¬ê°„ìœ¼ë¡œ ì´ë™")
#                     return total_results
#                 elif result == "ERROR":
#                     print("âš ï¸ ì˜¤ë¥˜ ë°œìƒ â†’ ë‹¤ìŒ ê¸€ë¡œ ê±´ë„ˆëœ€")
#                     continue
#                 else:
#                     total_results.append(result)
#                     print(f"ì €ì¥: {result['title'][:30]}...")

#         page += 1
#         time.sleep(0.3)

#     filename = f"ì •í˜•ì™¸ê³¼_{year}_{str(month).zfill(2)}_qna.json"
#     with open(filename, "w", encoding="utf-8") as f:
#         json.dump(total_results, f, ensure_ascii=False, indent=2)

#     print(f"\nâœ… {filename} ì €ì¥ ì™„ë£Œ (ì´ {len(total_results)}ê±´)\n")

# if __name__ == "__main__":
#     crawl_month(2024, 12)